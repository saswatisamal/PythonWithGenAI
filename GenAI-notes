Autoencoder :
Autoencoder consists of sequential models: encoder and decoder 
The goal of autoencoder is to regenerate the original image and it is validated against the original image.

CNN is used over ANN for images to save computation as large no of parameters needs to be calculated if we use ANN.


How CNN can be equivalent to ANN ?
A Neuron in ANN is equivalent to a filter or kernel in CNN.
The weights in ANN is equivalent to the elements of filter in CNN . [vertical ,horizontal]
In ANN we find the correct weights and biases ,similarly in CNN we find the correct filters that helps to detect the features of the image .
Bias is added in both ANN and CNN so that function doesn't always pass through origin in ANN and important feature is not ignored in case of CNN.


Difference:
Output of each epoch is a single o/p .
Output of each epoch is a feature map .
CNN has idea about neighbouring pixel but ANN doesn't have idea about neighbouring pixel as the neurons are flattened .
Detecting features is happening in convolution layer . Before the final encoded layer we get it is passed through the Dense layer and dense layer 
expects the data to be flattened .
(Filter height*Filter width*inputchannel*outputchannel)+bias

weights and biases are changing a lot : exploding gradient [slow convergence]
weights and biases not changing : vanishing  gradient

Different types of generative models:
GAN: Generative Adversarial n/w   [ Generator , Discriminator]
VAE : Variational autoencoder 
Diffusion model 

Start with VAE : 1:23:08 hrs 


Different kinds of layers:
Dense layer : Fully connected .
Flatten layer: Any dimension to 1D .
Convolution 2D: Detect features 
Convolution 2D transponse: Upsampling 
Reshape : Has 0 parameters 

How to start creating models ?
Sequential model with dense layers .
Embedding layer : To give meaning to tokens 

If dense layer is not performing well??
Add dropout layers [Switch off neurons randomly so that it adds randomness]
These improves generalisation,avoids overfitting and applied at the time of training and ignored during inference .

Adding batch normalization (mean,standard deviation] layer to keep activation value/output of each layer under a particular range.

This helps in faster convergence [ Weights and biases don't change much]

If two models are trained on the same dataset, the one that achieves a target accuracy or loss in fewer epochs is said to have faster convergence.
Factors that affect convergence speed:

Learning rate – Too small → slow convergence, too large → unstable. A well-tuned rate accelerates convergence.

Weight initialization – Good initialization avoids vanishing/exploding gradients and helps optimization move quickly.

Optimizer choice – Adam, RMSprop, and momentum-based methods usually converge faster than vanilla SGD.

Batch normalization – Helps stabilize activations and speed up training.

Network architecture – Deeper/wider networks may converge slower if not designed properly.

Regularization – Dropout, L2, etc., can affect how smoothly the network converges.

Data preprocessing – Normalization and standardization of input data significantly improve convergence speed.



In Convolution2D we identify features but sometimes we need to downsample/blur the given image so we can use pooling layer .
Max Pooling/ Avg Pooling 


Discriminator is used in training phase but not in inference .
Generator winning but discriminator struggling [Mode collapse]:Generator producing same image again and again .
discriminator is winning and geneartor is failing then geneartor weights and biases needs to be updated and if discriminator is failing the 
discriminator weights and biases should be updated.
End goal is during inference the genrator should generate such fake images which appear real .

Latent space :
In neural networks, latent space is a compressed representation of your input data, where the network encodes the most important features in fewer dimensions.

VAE: variational autoencoder 




17/09/2025:










